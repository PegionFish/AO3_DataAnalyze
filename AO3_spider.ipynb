{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AO3文章下载器\n",
    "\n",
    "对AO3的爬取并不复杂，但还是有一些难度的。经过我一系列的测试发现,Request并不能有效的爬取AO3的信息，因此就选择了selenium。对于Selenium的介绍，我就过多不多描述了，有很多技术文介绍。大家可以自行搜索。我这里给大家一些关键词，方便大家搜索。\n",
    "\n",
    "* selenium配置chromedriver\n",
    "* selenium元素定位方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取AO3的中文文章其实并不复杂，\n",
    "\n",
    " 1. 我们进入AO3首页，点击 “Search” 然后点击 “Edit Your Search” 进入高级搜索模式\n",
    " ![进入搜索](./img/search.jpg)\n",
    " 2. 在Language里选择中文，点击“Search”\n",
    " ![高级搜索](./img/gaojisearch.jpg)\n",
    " 3. 把滚动条拉倒最下面，点第二页\n",
    " 4. 把浏览起里面的地址复制下来\n",
    "  ![第二页](./img/page2.jpg)\n",
    "\n",
    " 我们仔细查看这个url请求，发现这个请求的参数还是非常清晰的，让我们来看看works/后面的参数：\n",
    " \n",
    " `search?commit=Search&page=`后面跟着一个数字2，我们点击其第三页，这个数字也变成了3。所以可以断定这个参数指的是页码  \n",
    " `&work_search%5Blanguage_id%5D=` 后面跟着zh字样，可以判这个参数是控制语言的  \n",
    " 同理`&work_search%5Brating_ids%5D=`控制的是分级  \n",
    " \n",
    " 其他字段也是类似的，大家有兴趣可以自己试验，我就不再叙述了，值得一提的是，我在爬取时并没有用到分级标签功能，只是在搜索里面翻文章。通过多次搜索我发现AO3的搜索结果有一定的随机性，并没有主动干预搜索结果，这一点还是很良心的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取搜索页面\n",
    "def make_search_url(page=1, langu=\"zh\", rating_key=\"\"):\n",
    "    rating = {\n",
    "    \"\": \"\",\n",
    "    \"Not_Rated\": 9,\n",
    "    \"General_Audiences\": 10, #一般观众\n",
    "    \"Teen_And_Up_Audiences\": 11, #青少年及以上观众\n",
    "    \"Mature\": 12, #成熟\n",
    "    \"Explicit\": 13, #明确的\n",
    "    }\n",
    "\n",
    "    base_loc = 'https://archiveofourown.org/works/'\n",
    "\n",
    "    base_loc += \"search?commit=Search&page=\"+str(page)+\"&utf8=%E2%9C%93\" #搜索页\n",
    "    base_loc += \"&work_search%5Bbookmarks_count%5D=\"\n",
    "    base_loc += \"&work_search%5Bcharacter_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bcomments_count%5D=\"\n",
    "    base_loc += \"&work_search%5Bcomplete%5D=\"\n",
    "    base_loc += \"&work_search%5Bcreators%5D=\"\n",
    "    base_loc += \"&work_search%5Bcrossover%5D=\"\n",
    "    base_loc += \"&work_search%5Bfandom_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bfreeform_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bhits%5D=\"\n",
    "    base_loc += \"&work_search%5Bkudos_count%5D=\"\n",
    "    base_loc += \"&work_search%5Blanguage_id%5D=\" + langu #语言\n",
    "    base_loc += \"&work_search%5Bquery%5D=\"\n",
    "    base_loc += \"&work_search%5Brating_ids%5D=\" + rating[rating_key] #分级\n",
    "    base_loc += \"&work_search%5Brelationship_names%5D=\"\n",
    "    base_loc += \"&work_search%5Brevised_at%5D=\"\n",
    "    base_loc += \"&work_search%5Bsingle_chapter%5D=0\"\n",
    "    base_loc += \"&work_search%5Bsort_column%5D=_score\"\n",
    "    base_loc += \"&work_search%5Bsort_direction%5D=desc\"\n",
    "    base_loc += \"&work_search%5Btitle%5D=\"\n",
    "    base_loc += \"&work_search%5Bword_count%5D=\"\n",
    "\n",
    "    return base_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看看搜索页面的html,在chrome中可以按F12打开开发者工具。 `Ctrl+Shift+C` 使用元素选择工具点击一下文章标题，查看后发现所有的搜索结果都在 `<ol class=work index group>`标签下，并且在`li`标签的`id`中记录了文章的id。\n",
    "![页面分析](./img/worklish.png)\n",
    "我们点击进入一篇文章，查看文章的url发现文章url与上面的id是一一对应的。这样，我们就可以通过分析搜索页得到文章的地址。\n",
    "![文章页面](./img/workurl.png)\n",
    "通过BeautifulSoup抓取相应标签获得`li`标签的`id`,就可以得到该搜索页下面所有的文章地址了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取文章链接\n",
    "def get_work_id_from_search(html):\n",
    "    old_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ol = soup.find('ol', attrs={'class': 'work index group'})\n",
    "    work_blurb_groups = ol.findAll('li', attrs={'class': 'work blurb group'})\n",
    "    for wbg in work_blurb_groups:\n",
    "        if wbg[\"id\"] not in old_list:\n",
    "            old_list.append(wbg[\"id\"])\n",
    "    \n",
    "    return old_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做好这些准备工作下面我们就开始正式爬取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fulltext/\"\n",
    "\n",
    "#5000页中文内容,这里先取500页测试\n",
    "start_p = 1\n",
    "end_p = 5000\n",
    "\n",
    "pbar = tqdm(range(start_p, end_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体思路是这样的：\n",
    "1. 打开一个浏览器，需要注意的我这里使用了代理，否则无法浏览到AO3；\n",
    "2. 通过selenium `find_element_by_id` 功能找到相应按钮自动点击，同意网站条款；\n",
    "3. 进入循环，通过`make_search_url`函数组合出搜索页的链接，遍历页码；\n",
    "4. 将搜索页的html传入函数`get_work_id_from_search`提取出所有文章id；\n",
    "5. 遍历文章id通过文章id组合出文章地址并访问，最后保存文章页面的html。\n",
    "\n",
    "这其中有两个注意事项：\n",
    "1. 当进入限制级文章时，网站会提示再次同意浏览条款，当检测到条款关键字时，使用`find_element_by_link_text('Proceed').click()`点击确认即可；\n",
    "2. 频繁访问后，网站会拒绝访问请求出现‘Retry later’页面，当检测到这种情况后，进行异常处理，关闭当前的浏览器，等待一分钟后重新访问。（这也是爬取文章速度比较慢的原因，有大神知道怎么解决的请赐教）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_service = webdriver.chrome.service.Service('/usr/bin/chromedriver')\n",
    "c_service.command_line_args()\n",
    "c_service.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=socks5://localhost:1080')\n",
    "\n",
    "browser = webdriver.Chrome(chrome_options=chrome_options)  # 调用Chrome浏览器\n",
    "\n",
    "browser.get(\"https://archiveofourown.org/\")\n",
    "\n",
    "time.sleep(3)\n",
    "browser.find_element_by_id('tos_agree').click()\n",
    "time.sleep(1)\n",
    "browser.find_element_by_id('accept_tos').click()\n",
    "time.sleep(1)\n",
    "\n",
    "for page in pbar:\n",
    "    search_url = make_search_url(page) #生成搜寻页面链接\n",
    "    browser.get(search_url)  # 请求页面，打开一个浏览器\n",
    "\n",
    "    html_text = browser.page_source  # 获得页面代码\n",
    "    try:\n",
    "        work_list = get_work_id_from_search(html_text) #获得文章的id\n",
    "        for work in work_list:\n",
    "            work_path = os.path.join(save_path, work+\".html\")\n",
    "\n",
    "            if os.path.exists(work_path):\n",
    "                continue\n",
    "            work_url = \"https://archiveofourown.org/works/\" + work.split(\"_\")[1] #创建文章URL\n",
    "            browser.get(work_url)\n",
    "            \n",
    "            html_text = browser.page_source  #获得页面代码\n",
    "            if \"If you accept cookies from our site and you choose \\\"Proceed\\\"\" in html_text: #无法获取正文则点击Proceed\n",
    "                browser.find_element_by_link_text('Proceed').click()\n",
    "                time.sleep(1)\n",
    "                browser.get(work_url)\n",
    "                html_text = browser.page_source\n",
    "\n",
    "            if \"Retry later\" in html_text:\n",
    "                raise AttributeError\n",
    "            if \"<!--chapter content-->\" in html_text:\n",
    "                pbar.set_description(\"saving: \" + work)\n",
    "                fh = open(work_path, 'w') #保存页面\n",
    "                fh.write(html_text) #写入内容\n",
    "                fh.close() #关闭\n",
    "            time.sleep(float(random.randint(10,50))/10) #随机延时\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        time.sleep(3)\n",
    "        browser.get(\"http://www.baidu.com\")\n",
    "        time.sleep(3)\n",
    "        browser.quit()\n",
    "        c_service.stop()\n",
    "        time.sleep(60)\n",
    "        c_service.start()\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)  #调用Chrome浏览器\n",
    "        browser.get(\"https://archiveofourown.org/\")\n",
    "        time.sleep(5)\n",
    "        browser.find_element_by_id('tos_agree').click()\n",
    "        time.sleep(2)\n",
    "        browser.find_element_by_id('accept_tos').click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "    time.sleep(float(random.randint(10,50))/10) #随机延时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
